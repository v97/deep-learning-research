{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 26\n",
      "2 610\n",
      "3 4632\n",
      "4 11144\n",
      "5 22939\n",
      "6 39511\n",
      "7 52087\n",
      "8 61009\n",
      "9 61755\n",
      "10 54321\n",
      "11 46410\n",
      "12 37524\n",
      "13 27924\n",
      "14 19258\n",
      "15 12148\n",
      "16 7114\n",
      "17 3982\n",
      "18 2003\n",
      "19 1053\n",
      "20 506\n",
      "21 238\n",
      "22 102\n",
      "23 49\n",
      "24 18\n",
      "25 7\n",
      "26 2\n",
      "27 3\n",
      "28 2\n",
      "29 2\n",
      "30 0\n",
      "31 1\n",
      "32 0\n",
      "33 0\n",
      "34 0\n",
      "35 0\n",
      "36 0\n",
      "37 0\n",
      "38 0\n",
      "39 0\n",
      "40 0\n",
      "41 0\n",
      "42 0\n",
      "43 0\n",
      "44 0\n",
      "45 1\n"
     ]
    }
   ],
   "source": [
    "file = open('words.txt')\n",
    "words = []\n",
    "word_embeddings = []\n",
    "i = 0\n",
    "for line in file:\n",
    "        if not re.search('[/&\\d]', line):\n",
    "            if(len(line) > 1 and line[-2] == '!'):\n",
    "                line = line[:-1]\n",
    "            word = line[:-1].lower() + ' '\n",
    "            words.append(word)\n",
    "unique_chars = set(''.join(words))\n",
    "ref = dict(zip(unique_chars, np.arange(len(unique_chars))))\n",
    "backref = dict(zip(ref.values(), ref.keys()))\n",
    "word_embeddings = [[ref[char] for char in word] for word in words]\n",
    "#[print(''.join([backref[index] for index in word])) for word in word_embeddings[1000:1010]]\n",
    "max_word_len = np.max([len(word) for word in words])\n",
    "words_by_len = [list([]) for len in range(max_word_len)]\n",
    "for word in word_embeddings:\n",
    "    words_by_len[len(word)-1].append(word)\n",
    "for length, words_of_length in enumerate(words_by_len):\n",
    "    print(length, len(words_of_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "num_classes = len(unique_chars)\n",
    "num_layers = 5\n",
    "state_size = 20\n",
    "batch_size = 5\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, None])\n",
    "y = tf.placeholder(tf.int32, [batch_size, None])\n",
    "state = tf.placeholder(tf.float32, [num_layers, 2, batch_size, state_size])\n",
    "state_tuple = tuple([tf.contrib.rnn.LSTMStateTuple(layer_state[0], layer_state[1]) for layer_state in tf.unstack(state, axis = 0)])\n",
    "\n",
    "cell = lambda : tf.contrib.rnn.BasicLSTMCell(state_size, state_is_tuple = True)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(cell(), 0.5) for layer in range(num_layers)], state_is_tuple = True)\n",
    "output_state, current_state = tf.nn.dynamic_rnn(cell, tf.cast(tf.expand_dims(x, -1), tf.float32), initial_state = state_tuple)\n",
    "\n",
    "W = tf.Variable(np.random.random([state_size, num_classes]), dtype = tf.float32)\n",
    "b = tf.Variable(np.zeros([1, num_classes]), dtype = tf.float32)\n",
    "\n",
    "logits = tf.matmul(tf.reshape(output_state, [-1, state_size]), W) + b\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = tf.reshape(y, [-1]))\n",
    "loss = tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(0.03).minimize(loss)\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for words of length: 45\n",
      "Training for words of length: 44\n",
      "Training for words of length: 43\n",
      "Training for words of length: 42\n",
      "Training for words of length: 41\n",
      "Training for words of length: 40\n",
      "Training for words of length: 39\n",
      "Training for words of length: 38\n",
      "Training for words of length: 37\n",
      "Training for words of length: 36\n",
      "Training for words of length: 35\n",
      "Training for words of length: 34\n",
      "Training for words of length: 33\n",
      "Training for words of length: 32\n",
      "Training for words of length: 31\n",
      "Training for words of length: 30\n",
      "Training for words of length: 29\n",
      "Training for words of length: 28\n",
      "Training for words of length: 27\n",
      "Training for words of length: 26\n",
      "Training for words of length: 25\n",
      "Training for words of length: 24\n",
      "Training for words of length: 23\n",
      "Training for words of length: 22\n",
      "Training for words of length: 21\n",
      "Training for words of length: 20\n",
      "Trained: 100 \tLoss:  2.9044\n",
      "Training for words of length: 19\n",
      "Trained: 200 \tLoss:  2.67944\n",
      "Trained: 300 \tLoss:  2.56225\n",
      "Training for words of length: 18\n",
      "Trained: 400 \tLoss:  2.56801\n",
      "Trained: 500 \tLoss:  2.54874\n",
      "Trained: 600 \tLoss:  2.45966\n",
      "Trained: 700 \tLoss:  2.53377\n",
      "Training for words of length: 17\n",
      "Trained: 800 \tLoss:  2.5467\n",
      "Trained: 900 \tLoss:  2.47152\n",
      "Trained: 1000 \tLoss:  2.51027\n",
      "Trained: 1100 \tLoss:  2.48924\n",
      "Trained: 1200 \tLoss:  2.31886\n",
      "Trained: 1300 \tLoss:  2.46118\n",
      "Trained: 1400 \tLoss:  2.47562\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    _state = np.zeros([num_layers, 2, batch_size, state_size], dtype = np.float32)\n",
    "    count = 0\n",
    "    losses = []\n",
    "    for word_len in range(max_word_len - 1, 0, -1):\n",
    "        print(\"Training for words of length:\", word_len)\n",
    "        num_words = len(words_by_len[word_len])\n",
    "        for index in range(0, num_words, 5):\n",
    "            if( (num_words - 1) - index + 1 < 5):\n",
    "                break\n",
    "            _x = words_by_len[word_len][index : index + 5]\n",
    "            _y = np.roll(_x, -1, axis = 1)\n",
    "            _y[:, -1] = ref[' ']\n",
    "            _train, _loss = sess.run([train_op, loss], {x: _x, y: _y, state: _state})\n",
    "            count += 1\n",
    "            losses.append(_loss)\n",
    "            if(count % 100 == 0):\n",
    "                print(\"Trained:\", count, \"\\tLoss: \", np.mean(losses))\n",
    "                losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
